{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install pytorch_forecasting wandb pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['WANDB_NOTEBOOK_NAME'] = 'pytorch_stats_own_data.ipynb'\n",
    "os.environ['WANDB_API_KEY'] = 'f59f037973c4de4681bc9330ea3a477cb0328ca5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPUs: 2\n",
      "Available CPUs: 32\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "import torch\n",
    "\n",
    "from pytorch_forecasting import Baseline, NBeats, TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import NaNLabelEncoder\n",
    "from pytorch_forecasting.metrics import SMAPE\n",
    "\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "# wandb_logger = WandbLogger(project=\"Digital-Energy\")\n",
    "\n",
    "import os\n",
    "AVAILABLE_GPUS = torch.cuda.device_count()\n",
    "AVAILABLE_CPUS = os.cpu_count()\n",
    "\n",
    "print(f\"Available GPUs: {AVAILABLE_GPUS}\")\n",
    "print(f\"Available CPUs: {AVAILABLE_CPUS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Own dataset\n",
    "\n",
    "We load in all the data from the london dataset for fast testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing on 1 file\n",
    "\n",
    "file = \"./Data/london/household_MAC000002.csv\"\n",
    "data = pd.read_csv(file, parse_dates=[\"DateTime\"])\n",
    "data = data.drop(columns=[\"stdorToU\", \"timeOfDay\", \"Unnamed: 0\"])\n",
    "data = data.rename(columns={\"KWH/hh (per half hour) \": \"KWH\"})\n",
    "data[\"time_idx\"] = data.index\n",
    "# drop missing values of KWH\n",
    "data = data.dropna(subset=[\"KWH\"]).reset_index(drop=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read all files in data folder\n",
    "\n",
    "here we read all the CSV files in the data folder and create our dataframe\n",
    "\n",
    "This dataframe has more then a Billion rows (with a B) so it will takes some time. 13 min on a 2018 macbook pro 15'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "# load all csv files in london folder into 1 dataframe\n",
    "data = pd.concat([pd.read_csv(f) for f in sorted(glob.glob(\"data/london/*.csv\"))])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean dataframe. Here we drop the columns that are taking up space and rename the WKH column for easy querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=[\"stdorToU\", \"timeOfDay\", \"Unnamed: 0\"])\n",
    "data = data.rename(columns={\"KWH/hh (per half hour) \": \"KWH\"})\n",
    "data[\"time_idx\"] = data.index\n",
    "# drop missing values of KWH\n",
    "data = data.dropna(subset=[\"KWH\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save dataframe in more performant format (parquet)\n",
    "\n",
    "Due to the size of the data i did some research on the best format to save the data.\n",
    "\n",
    "here the results show that the parquet and feather formats are the fastest to read and write. \n",
    "\n",
    "\n",
    "with parquet being the fastest overall while taking up the least disk space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Comented for safety\n",
    "# data.to_parquet(\"data/ldn_df.parquet\")\n",
    "# data.to_parquet(\"data/ldn_df_small.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_parquet(\"data/ldn_df.parquet\")\n",
    "data = pd.read_parquet(\"../../Data/ldn_df_small.parquet\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"DateTime\"] = pd.to_datetime(data[\"DateTime\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding time_idx Column\n",
    "\n",
    "we add this column to the dataframe so pytorch-forecasting can use the time_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all unique households\n",
    "households = data[\"LCLid\"].unique()\n",
    "\n",
    "# find highest count of \"LCLid\" unique values and create a range from it\n",
    "rows_of_households = data[\"LCLid\"].value_counts().sort_index()\n",
    "rows_of_households = [range(1, x + 1) for x in rows_of_households.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add households and ranges to a dictionary\n",
    "dict = {k:v for k,v in zip(households, rows_of_households)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_output(r):\n",
    "    r['time_idx_2'] = dict[r.iloc[0]['LCLid']]\n",
    "    return r\n",
    "\n",
    "\n",
    "df = data.groupby('LCLid').apply(fill_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "for i in rows_of_households:\n",
    "    x += [i[-1]]\n",
    "\n",
    "min(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting training, we need to split the dataset into a training and validation TimeSeriesDataSet.\n",
    "\n",
    "here are the parameters used for the TimeSeriesDataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset and dataloaders\n",
    "max_prediction_length = 48\n",
    "max_encoder_length = max_prediction_length * 5\n",
    "\n",
    "training_cutoff = 19523 - max_prediction_length\n",
    "\n",
    "context_length = max_encoder_length\n",
    "prediction_length = max_prediction_length\n",
    "\n",
    "training = TimeSeriesDataSet(\n",
    "    data[lambda x: x.time_idx <= training_cutoff],\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"KWH\",\n",
    "    categorical_encoders={\"LCLid\": NaNLabelEncoder().fit(data.LCLid)},\n",
    "    group_ids=[\"LCLid\"],\n",
    "    # only unknown variable is \"KWH\" - and N-Beats can also not take any additional variables\n",
    "    time_varying_unknown_reals=[\"KWH\"],\n",
    "    max_encoder_length=context_length,\n",
    "    max_prediction_length=prediction_length,\n",
    "    allow_missing_timesteps=True,\n",
    ")\n",
    "\n",
    "validation = TimeSeriesDataSet.from_dataset(training, data, min_prediction_idx=training_cutoff + 1)\n",
    "batch_size = 400\n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=AVAILABLE_CPUS)\n",
    "val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size, num_workers=AVAILABLE_CPUS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate baseline error\n",
    "\n",
    "Our baseline model predicts future values by repeating the last know value. The resulting SMAPE is disappointing and should not be easy to beat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate baseline absolute error\n",
    "actuals = torch.cat([y[0] for x, y in iter(val_dataloader)])\n",
    "baseline_predictions = Baseline().predict(val_dataloader)\n",
    "SMAPE()(baseline_predictions, actuals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train network\n",
    "\n",
    "Finding the optimal learning rate using [PyTorch Lightning](https://pytorch-lightning.readthedocs.io/) is easy. The key hyperparameter of the NBeats model are the widths. Each denotes the width of each forecasting block. By default, the first forecasts the trend, while the second forecasts seasonality.\n",
    "\n",
    "![test](https://miro.medium.com/max/836/1*1If8JU4JwFAta1kjMjkTQg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.seed_everything(42)\n",
    "trainer = pl.Trainer(gpus=1,gradient_clip_val=0.01, logger=wandb_logger)\n",
    "net = NBeats.from_dataset(training, learning_rate=3e-2, weight_decay=1e-2, widths=[32, 512], backcast_loss_ratio=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find optimal learning rate\n",
    "res = trainer.tuner.lr_find(net, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader, min_lr=1e-5)\n",
    "print(f\"suggested learning rate: {res.suggestion()}\")\n",
    "fig = res.plot(show=True, suggest=True)\n",
    "fig.show()\n",
    "net.hparams.learning_rate = res.suggestion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "t0 = time.time()\n",
    "\n",
    "\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=10, verbose=False, mode=\"min\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=100,\n",
    "    gpus=1,\n",
    "    weights_summary=\"top\",\n",
    "    gradient_clip_val=0.01,\n",
    "    callbacks=[early_stop_callback],\n",
    "    limit_train_batches=30,\n",
    "    logger=wandb_logger\n",
    ")\n",
    "\n",
    "\n",
    "net = NBeats.from_dataset(\n",
    "    training,\n",
    "    learning_rate=net.hparams.learning_rate if net.hparams.learning_rate else 0.02,\n",
    "    log_interval=10,\n",
    "    log_val_interval=1,\n",
    "    weight_decay=1e-2,\n",
    "    widths=[32, 512],\n",
    "    backcast_loss_ratio=1.0\n",
    ")\n",
    "\n",
    "trainer.fit(\n",
    "    net,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    ")\n",
    "\n",
    "print(f\"{time.time() - t0} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "best_model = NBeats.load_from_checkpoint(best_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We predict on the validation dataset with predict() and calculate the error which is well below the baseline error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actuals = torch.cat([y[0] for x, y in iter(val_dataloader)])\n",
    "predictions = best_model.predict(val_dataloader)\n",
    "(actuals - predictions).abs().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at random samples from the validation set is always a good way to understand if the forecast is reasonable - and it is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_predictions, x = best_model.predict(val_dataloader, mode=\"raw\", return_x=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(3):  # plot 10 examples\n",
    "    best_model.plot_prediction(x, raw_predictions, idx=idx, add_loss_to_title=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpret model\n",
    "\n",
    "We can ask PyTorch Forecasting to decompose the prediction into seasonality and trend with plot_interpretation(). This is a special feature of the NBeats model and only possible because of its unique architecture. The results show that there seem to be many ways to explain the data and the algorithm does not always chooses the one making intuitive sense. This is partially down to the small number of time series we trained on (100). But it is also due because our forecasting period does not cover multiple seasonalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(1):  # plot 10 examples\n",
    "    best_model.plot_interpretation(x, raw_predictions, idx=idx)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac2eaa0ea0ebeafcc7822e65e46aa9d4f966f30b695406963e145ea4a91cd4fc"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
