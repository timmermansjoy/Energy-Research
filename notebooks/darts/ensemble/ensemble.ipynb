{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"test_plot\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from multiprocessing.dummy import freeze_support\n",
    "import os\n",
    "import sys\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = 'n-beats.ipynb'\n",
    "os.environ['WANDB_API_KEY'] = os.getenv('WANDB_API_KEY')\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "from darts import TimeSeries\n",
    "from darts.models import NBEATSModel, TFTModel, RegressionEnsembleModel\n",
    "from darts.dataprocessing.transformers import Scaler, MissingValuesFiller\n",
    "from darts.metrics import mape, r2_score, rmse, mse\n",
    "\n",
    "from darts import TimeSeries\n",
    "\n",
    "from darts.datasets import EnergyDataset\n",
    "\n",
    "import helper\n",
    "import glob\n",
    "import wandb\n",
    "\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "from tqdm.contrib.concurrent import process_map\n",
    "import tqdm\n",
    "\n",
    "\n",
    "AVAILABLE_GPUS = torch.cuda.device_count()\n",
    "AVAILABLE_CPUS = os.cpu_count()\n",
    "\n",
    "print(f\"Available GPUs: {AVAILABLE_GPUS}\")\n",
    "print(f\"Available CPUs: {AVAILABLE_CPUS}\")\n",
    "\n",
    "# wandb.init(project=\"Digital-Energy\", name=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = pd.read_csv(\"../../../Data/London_weather_2011-2014.csv\")\n",
    "weather[\"DateTime\"] = pd.to_datetime(weather[\"DateTime\"])\n",
    "\n",
    "def reader(x):\n",
    "    df = pd.read_csv(x)\n",
    "    df[\"DateTime\"] = pd.to_datetime(df[\"DateTime\"])\n",
    "    df = weather.merge(df, on=\"DateTime\", how=\"right\")\n",
    "    df.fillna(method=\"ffill\" ,inplace=True)\n",
    "    series = TimeSeries.from_dataframe(df, time_col=\"DateTime\", value_cols=[\"KWHhh\"], freq=\"30min\", fill_missing_dates=True, fillna_value=True).astype(np.float32)\n",
    "    covarient = TimeSeries.from_dataframe(df, time_col=\"DateTime\", value_cols=[\"Temperature_C\", \"Humidity_%\", \"Dew_Point_C\"], freq=\"30min\", fill_missing_dates=True, fillna_value=True).astype(np.float32)\n",
    "    list = [series, covarient]\n",
    "    return list\n",
    "\n",
    "def splitter():\n",
    "    file_list = sorted(glob.glob(\"../../../Data/london_clean/*.csv\"))[:1]\n",
    "    if file_list == []:\n",
    "        raise Exception(\"No files found\")\n",
    "    return process_map(reader, file_list, chunksize=20)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    freeze_support()\n",
    "    my_time_series_dataset = splitter()\n",
    "\n",
    "    values_series = [x[0] for x in my_time_series_dataset]\n",
    "    covariance_series = [x[1] for x in my_time_series_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sets\n",
    "training_sets = []\n",
    "validation_sets = []\n",
    "for x in values_series:\n",
    "    train, val = x.split_after(0.85)\n",
    "    training_sets.append(train)\n",
    "    validation_sets.append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##normalize the data\n",
    "scaler = Scaler()\n",
    "\n",
    "train_transformed = []\n",
    "val_transformed = []\n",
    "for x in training_sets:\n",
    "    train_transformed.append(scaler.fit_transform(x))\n",
    "for x in validation_sets:\n",
    "    val_transformed.append(scaler.fit_transform(x))\n",
    "\n",
    "\n",
    "## covariate series\n",
    "cov_transformed = []\n",
    "for x in covariance_series:\n",
    "    cov_transformed.append(scaler.fit_transform(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_beats = NBEATSModel.load_from_checkpoint(work_dir=\"../../../Models/\", model_name=\"nbeats-3000\", best=True)\n",
    "# tft = TFTModel.load_from_checkpoint(work_dir=\"../../../Models/\", model_name=\"tft-correct-transform-weather-2000\", best=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_logger = WandbLogger(project=\"Digital-Energy\", log_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_beats = NBEATSModel(\n",
    "    input_chunk_length=96,\n",
    "    output_chunk_length=96,\n",
    "    generic_architecture=False,\n",
    "    #num_stacks=10,\n",
    "    num_blocks=3,\n",
    "    num_layers=5,\n",
    "    layer_widths=512,\n",
    "    n_epochs=5,\n",
    "    nr_epochs_val_period=1,\n",
    "    batch_size=64,\n",
    "    work_dir=\"../../../Models\",\n",
    "    save_checkpoints=False,\n",
    "    # model_name=MODEL_NAME,\n",
    "    pl_trainer_kwargs={\n",
    "    \"enable_progress_bar\": True,\n",
    "    \"enable_model_summary\": True,\n",
    "    \"accelerator\": \"gpu\",\n",
    "    \"devices\": 1,\n",
    "    \"logger\": wandb_logger,\n",
    "    },\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tft = TFTModel(\n",
    "    input_chunk_length=96,\n",
    "    output_chunk_length=96,\n",
    "    hidden_size=64,\n",
    "    lstm_layers=2,\n",
    "    num_attention_heads=4,\n",
    "    dropout=0.04,\n",
    "    batch_size=64,\n",
    "    n_epochs=1,\n",
    "    add_relative_index=True,\n",
    "    work_dir=\"../../../Models\",\n",
    "    save_checkpoints=False,\n",
    "    pl_trainer_kwargs={\n",
    "    \"enable_progress_bar\": True,\n",
    "    \"enable_model_summary\": True,\n",
    "    \"accelerator\": \"gpu\",\n",
    "    \"devices\": [1],\n",
    "    \"logger\": wandb_logger,\n",
    "    },\n",
    "    # likelihood=QuantileRegression(\n",
    "    #     quantiles=quantiles\n",
    "    # ),  # QuantileRegression is set per default\n",
    "    random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensamble = RegressionEnsembleModel(forecasting_models=[n_beats, tft], regression_train_n_points=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensamble.fit(series=train_transformed[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START = 20\n",
    "SAMPLES = 10\n",
    "for i, x in enumerate(sorted(glob.glob(\"../../../Data/london_clean/*.csv\"))[START:START+SAMPLES]):\n",
    "\n",
    "    df = pd.read_csv(x)\n",
    "    df[\"DateTime\"] = pd.to_datetime(df[\"DateTime\"])\n",
    "    df = weather.merge(df, on=\"DateTime\", how=\"right\")\n",
    "    df.fillna(method=\"ffill\" ,inplace=True)\n",
    "    series = TimeSeries.from_dataframe(df, time_col=\"DateTime\", value_cols=[\"KWHhh\"], freq=\"30min\", fill_missing_dates=True, fillna_value=True).astype(np.float32)\n",
    "    covarient = TimeSeries.from_dataframe(df, time_col=\"DateTime\", value_cols=[\"Temperature_C\", \"Humidity_%\", \"Dew_Point_C\"], freq=\"30min\", fill_missing_dates=True, fillna_value=True).astype(np.float32)\n",
    "    MID = len(series)//4\n",
    "    series = series[MID:MID+600]\n",
    "    series = scaler.fit_transform(series)\n",
    "\n",
    "\n",
    "\n",
    "    pred_series = ensamble.historical_forecasts(\n",
    "        series,\n",
    "        forecast_horizon=1,\n",
    "        stride=1,\n",
    "        retrain=False,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    print(f\"rmse: {rmse(series, pred_series)}.\")\n",
    "    print(f\"R2 score: {r2_score(series, pred_series)}.\")\n",
    "\n",
    "    # fig = helper.display_forecast(pred_series, series, \"1 day\", save=False, fig_name=f\"{x.split('/')[-1]}-test\", fig_size=(20,10))\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    plt.ylim(0, 3)\n",
    "    plt.title(f\"{MODEL_NAME} - {x.split('/')[-1]} - MSE: {mse(series, pred_series)}\")\n",
    "    series.plot()\n",
    "    pred_series.plot()\n",
    "    fig = plt\n",
    "    wandb.log({\n",
    "            \"mape\": mape(series, pred_series),\n",
    "            \"mse\": mse(series, pred_series),\n",
    "            \"rmse\": rmse(series, pred_series),\n",
    "            \"r2\": r2_score(series, pred_series),\n",
    "            \"result\": fig\n",
    "    })\n",
    "    plt.savefig(f\"../../../Plots/{MODEL_NAME}/{x.split('/')[-1]}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
