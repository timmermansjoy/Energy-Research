{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"test_plot\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-10 09:24:36 apex.transformer.tensor_parallel WARNING: `fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
      "2022-05-10 09:24:37 root WARNING: Bagua cannot detect bundled NCCL library, Bagua will try to use system NCCL instead. If you encounter any error, please run `import bagua_core; bagua_core.install_deps()` or the `bagua_install_deps.py` script to install bundled libraries.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPUs: 2\n",
      "Available CPUs: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtimmermansjoy\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/notebooks/darts/n-beats/wandb/run-20220510_092438-3usvcsz8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/timmermansjoy/digital-energy/runs/3usvcsz8\" target=\"_blank\">test_plot</a></strong> to <a href=\"https://wandb.ai/timmermansjoy/digital-energy\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/timmermansjoy/digital-energy/runs/3usvcsz8?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fb5031049a0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from multiprocessing.dummy import freeze_support\n",
    "import os\n",
    "import sys\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = 'n-beats.ipynb'\n",
    "os.environ['WANDB_API_KEY'] = os.getenv('WANDB_API_KEY')\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "from darts import TimeSeries\n",
    "from darts.models import NBEATSModel, NaiveDrift\n",
    "from darts.dataprocessing.transformers import Scaler, MissingValuesFiller\n",
    "from darts.metrics import mape, r2_score, rmse, mse\n",
    "\n",
    "from darts import TimeSeries\n",
    "\n",
    "from darts.datasets import EnergyDataset\n",
    "\n",
    "import helper\n",
    "import glob\n",
    "import wandb\n",
    "\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "from tqdm.contrib.concurrent import process_map\n",
    "import tqdm\n",
    "\n",
    "\n",
    "AVAILABLE_GPUS = torch.cuda.device_count()\n",
    "AVAILABLE_CPUS = os.cpu_count()\n",
    "\n",
    "print(f\"Available GPUs: {AVAILABLE_GPUS}\")\n",
    "print(f\"Available CPUs: {AVAILABLE_CPUS}\")\n",
    "\n",
    "wandb.init(project=\"Digital-Energy\", name=MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "Taking the first 300 households from the London Dataset and converting them to a Darts TimeSeries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_time_series_dataset = []\n",
    "# for x in tqdm.tqdm(sorted(glob.glob(\"../../../Data/london_clean/*.csv\"))[:1000]):\n",
    "#     df = pd.read_csv(f'{x}')\n",
    "#     df[\"DateTime\"] = pd.to_datetime(df['DateTime'])\n",
    "#     # df = df.groupby(pd.Grouper(key='DateTime', freq='1D')).max(\"KWHhh\").round(3).reset_index()\n",
    "#     series = TimeSeries.from_dataframe(df, time_col='DateTime', value_cols='KWHhh').astype(np.float32)\n",
    "#     #plot series and save\n",
    "#     series.plot()\n",
    "#     plt.savefig(f'../../../Plots/testing/{x.split(\"/\")[-1]}.png')\n",
    "#     plt.close()\n",
    "\n",
    "\n",
    "#     my_time_series_dataset.append(series)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reader(x):\n",
    "    return TimeSeries.from_csv(x, time_col='DateTime', value_cols='KWHhh',\n",
    "                            fill_missing_dates=True, fillna_value=True, freq=\"30min\").astype(np.float32)\n",
    "\n",
    "\n",
    "def splitter():\n",
    "    file_list = sorted(glob.glob(\"../../../Data/london_clean/*.csv\"))[:1000]\n",
    "    if file_list == []:\n",
    "        raise Exception(\"No files found\")\n",
    "    return process_map(reader, file_list, chunksize=5)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    freeze_support()\n",
    "    my_time_series_dataset = splitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "my_time_series_dataset[random.randint(0, len(my_time_series_dataset))].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sets\n",
    "training_sets = []\n",
    "validation_sets = []\n",
    "for x in my_time_series_dataset:\n",
    "    train, val = x.split_after(0.75)\n",
    "    training_sets.append(train)\n",
    "    validation_sets.append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in my_time_series_dataset:\n",
    "#     helper.find_gaps(i.pd_dataframe(), delta=60*24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "We create a N-Beats model that utilizes the GPU, Weights, Biases logger and early stopping callback.\n",
    "\n",
    "## Early stopping\n",
    "\n",
    "An early stopping callback is used to stop the training if the validation loss does not improve after a certain number of epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=0.005,\n",
    "    patience=15,\n",
    "    verbose=True,\n",
    "    mode=\"min\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoders = {\n",
    "    # \"datetime_attribute\": {\"future\": [\"DateTime\"], \"past\": [\"DateTime\"]},\n",
    "    \"position\": {\"past\": [\"absolute\"]},\n",
    "    \"transformer\": Scaler(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_logger = WandbLogger(project=\"Digital-Energy\", log_model=True)\n",
    "\n",
    "\n",
    "# input chunk = The length of the input sequence fed to the model\n",
    "# output chunk = The length of the output sequence predicted by the model\n",
    "\n",
    "model_nbeats = NBEATSModel(\n",
    "    input_chunk_length=3,\n",
    "    output_chunk_length=1,\n",
    "    generic_architecture=False,\n",
    "    #num_stacks=10,\n",
    "    num_blocks=3,\n",
    "    num_layers=5,\n",
    "    layer_widths=512,\n",
    "    n_epochs=25,\n",
    "    nr_epochs_val_period=1,\n",
    "    batch_size=256,\n",
    "    work_dir=\"../../../Models\",\n",
    "    save_checkpoints=False,\n",
    "    # model_name=MODEL_NAME,\n",
    "    pl_trainer_kwargs={\n",
    "    \"enable_progress_bar\": True,\n",
    "    \"enable_model_summary\": True,\n",
    "    \"accelerator\": \"gpu\",\n",
    "    \"devices\": 1,\n",
    "    \"logger\": wandb_logger,\n",
    "    \"callbacks\": [early_stop_callback],\n",
    "    },\n",
    "    # loss_fn=torch.nn.CrossEntropyLoss() # custom loss function\n",
    "    # optimizer_cls=torch.optim.Adam,\n",
    "    # add_encoders=encoders,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_time_series_dataset[551].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wandb_logger.watch(model_nbeats) # sadly this feature does not work for Darts models\n",
    "model_nbeats.fit(series=training_sets, val_series=validation_sets, num_loader_workers=AVAILABLE_CPUS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(\"../../../Data/london_clean/*.csv\")\n",
    "df = pd.read_csv(files[2000])\n",
    "df[\"DateTime\"] = pd.to_datetime(df['DateTime'])\n",
    "df = df.groupby(pd.Grouper(key='DateTime', freq='1D')).max(\"KWHhh\").round(3).reset_index()\n",
    "series = TimeSeries.from_dataframe(df, value_cols=['KWHhh'], time_col=\"DateTime\").astype(np.float32)\n",
    "series = series[-200:]\n",
    "\n",
    "\n",
    "pred_series = model_nbeats.historical_forecasts(\n",
    "    series,\n",
    "    forecast_horizon=1,\n",
    "    stride=1,\n",
    "    retrain=False,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series.plot()\n",
    "pred_series.plot()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_model = NaiveDrift(K=1)\n",
    "naive_model.fit(train)\n",
    "naive_forecast = naive_model.predict(series)\n",
    "naive_forecast.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([1, 2, 3, 4])\n",
    "plt.ylabel(\"some interesting numbers\")\n",
    "wandb.log({\"chart\": plt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot train and validation series with matplotlib\n",
    "\n",
    "plt.figure(figsize=(20,15))\n",
    "plt.ylim(0, 3)\n",
    "training_sets[1].plot(label=(\"train\"))\n",
    "validation_sets[1].plot(label=(\"val\"))\n",
    "# show plot\n",
    "fig = helper.display_forecast(training_sets[1], training_sets[2], \"1day\", save=False)\n",
    "plt.legend()\n",
    "wandb.log({\"chart_test\": plt,\n",
    "           \"helper\": fig})\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START = 3000\n",
    "for i, x in enumerate(sorted(glob.glob(\"../../../Data/london_clean/*.csv\"))[START:START+10]):\n",
    "\n",
    "    df = pd.read_csv(x)\n",
    "    df[\"DateTime\"] = pd.to_datetime(df['DateTime'])\n",
    "    series = TimeSeries.from_dataframe(df, value_cols=['KWHhh'], time_col=\"DateTime\").astype(np.float32)\n",
    "    series = series[-400:]\n",
    "\n",
    "\n",
    "    pred_series = model_nbeats.historical_forecasts(\n",
    "        series,\n",
    "        forecast_horizon=1,\n",
    "        stride=1,\n",
    "        retrain=False,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    print(f\"rmse: {rmse(series, pred_series)}.\")\n",
    "    print(f\"R2 score: {r2_score(series, pred_series)}.\")\n",
    "\n",
    "    fig = helper.display_forecast(pred_series, series, \"1 day\", save=True, fig_name=f\"{i}\", model_name=\"test\", fig_size=(20,10))\n",
    "\n",
    "    wandb.log({\n",
    "            \"mape\": mape(series, pred_series),\n",
    "            \"mse\": mse(series, pred_series),\n",
    "            \"rmse\": rmse(series, pred_series),\n",
    "            \"r2\": r2_score(series, pred_series),\n",
    "            \"result\": fig\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading checkpoints of the model\n",
    "\n",
    "loading the best checkpoint of the model. To compare the results of the model with the previous one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "model_nbeats = NBEATSModel.load_from_checkpoint(work_dir=\"../../../Models/\", model_name=\"n_beats_biggysmall\", best=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START = 3000\n",
    "for i, x in enumerate(sorted(glob.glob(\"../../Data/london_clean/*.csv\"))[START:START+10]):\n",
    "\n",
    "    df = pd.read_csv(x)\n",
    "    df[\"DateTime\"] = pd.to_datetime(df['DateTime'])\n",
    "    series = TimeSeries.from_dataframe(df, value_cols=['KWHhh'], time_col=\"DateTime\", fill_missing_dates=True, freq=\"30min\").astype(np.float32)\n",
    "    series = series[-600:]\n",
    "\n",
    "\n",
    "    pred_series = model_nbeats.predict(\n",
    "        1,\n",
    "        series,\n",
    "    )\n",
    "\n",
    "    print(f\"rmse: {rmse(series, pred_series)}.\")\n",
    "    print(f\"R2 score: {r2_score(series, pred_series)}.\")\n",
    "\n",
    "    helper.display_forecast(pred_series, series, \"1 day\", save=True, fig_name=f\"{i}\", fig_size=(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.display_forecast(pred_series, series, \"1 day\", save=False, fig_name=f\"test\", model_name=f\"{MODEL_NAME}\", fig_size=(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
