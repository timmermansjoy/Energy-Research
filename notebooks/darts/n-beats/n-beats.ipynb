{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"n-beats-hpam-v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Available GPUs: 2\n",
      "Available CPUs: 32\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from multiprocessing.dummy import freeze_support\n",
    "import os\n",
    "import sys\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = 'n-beats.ipynb'\n",
    "os.environ['WANDB_API_KEY'] = os.getenv('WANDB_API_KEY')\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "from darts import TimeSeries\n",
    "from darts.models import NBEATSModel\n",
    "from darts.dataprocessing.transformers import Scaler, MissingValuesFiller\n",
    "from darts.metrics import mape, r2_score, rmse\n",
    "\n",
    "from darts import TimeSeries\n",
    "\n",
    "from darts.datasets import EnergyDataset\n",
    "\n",
    "import helper\n",
    "import glob\n",
    "import progressbar\n",
    "\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "\n",
    "AVAILABLE_GPUS = torch.cuda.device_count()\n",
    "AVAILABLE_CPUS = os.cpu_count()\n",
    "\n",
    "print(f\"Available GPUs: {AVAILABLE_GPUS}\")\n",
    "print(f\"Available CPUs: {AVAILABLE_CPUS}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "Taking the first 300 households from the London Dataset and converting them to a Darts TimeSeries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% (0 of 1000) |                       | Elapsed Time: 0:00:00 ETA:  --:--:--\n",
      "  1% (10 of 1000) |                      | Elapsed Time: 0:00:00 ETA:   0:00:10\n",
      "  2% (20 of 1000) |                      | Elapsed Time: 0:00:00 ETA:   0:00:10\n",
      "  2% (26 of 1000) |                      | Elapsed Time: 0:00:00 ETA:   0:00:09\n",
      "  3% (36 of 1000) |                      | Elapsed Time: 0:00:00 ETA:   0:00:09\n",
      "  4% (46 of 1000) |#                     | Elapsed Time: 0:00:00 ETA:   0:00:09\n",
      "  5% (51 of 1000) |#                     | Elapsed Time: 0:00:00 ETA:   0:00:09\n",
      "  6% (61 of 1000) |#                     | Elapsed Time: 0:00:00 ETA:   0:00:09\n",
      "  7% (71 of 1000) |#                     | Elapsed Time: 0:00:00 ETA:   0:00:09\n",
      "  7% (76 of 1000) |#                     | Elapsed Time: 0:00:00 ETA:   0:00:09\n",
      "  8% (86 of 1000) |#                     | Elapsed Time: 0:00:00 ETA:   0:00:09\n",
      "  9% (96 of 1000) |##                    | Elapsed Time: 0:00:00 ETA:   0:00:09\n",
      " 10% (102 of 1000) |##                   | Elapsed Time: 0:00:01 ETA:   0:00:09\n",
      " 11% (112 of 1000) |##                   | Elapsed Time: 0:00:01 ETA:   0:00:09\n",
      " 12% (122 of 1000) |##                   | Elapsed Time: 0:00:01 ETA:   0:00:08\n",
      " 12% (127 of 1000) |##                   | Elapsed Time: 0:00:01 ETA:   0:00:08\n",
      " 13% (137 of 1000) |##                   | Elapsed Time: 0:00:01 ETA:   0:00:08\n",
      " 14% (147 of 1000) |###                  | Elapsed Time: 0:00:01 ETA:   0:00:08\n",
      " 15% (152 of 1000) |###                  | Elapsed Time: 0:00:01 ETA:   0:00:08\n",
      " 16% (162 of 1000) |###                  | Elapsed Time: 0:00:01 ETA:   0:00:08\n",
      " 17% (172 of 1000) |###                  | Elapsed Time: 0:00:01 ETA:   0:00:08\n",
      " 17% (178 of 1000) |###                  | Elapsed Time: 0:00:01 ETA:   0:00:08\n",
      " 18% (188 of 1000) |###                  | Elapsed Time: 0:00:01 ETA:   0:00:08\n",
      " 19% (198 of 1000) |####                 | Elapsed Time: 0:00:02 ETA:   0:00:08\n",
      " 20% (209 of 1000) |####                 | Elapsed Time: 0:00:02 ETA:   0:00:07\n",
      " 21% (216 of 1000) |####                 | Elapsed Time: 0:00:02 ETA:   0:00:07\n",
      " 22% (226 of 1000) |####                 | Elapsed Time: 0:00:02 ETA:   0:00:07\n",
      " 23% (236 of 1000) |####                 | Elapsed Time: 0:00:02 ETA:   0:00:07\n",
      " 24% (241 of 1000) |#####                | Elapsed Time: 0:00:02 ETA:   0:00:07\n",
      " 25% (251 of 1000) |#####                | Elapsed Time: 0:00:02 ETA:   0:00:07\n",
      " 26% (261 of 1000) |#####                | Elapsed Time: 0:00:02 ETA:   0:00:07\n",
      " 26% (266 of 1000) |#####                | Elapsed Time: 0:00:02 ETA:   0:00:07\n",
      " 27% (276 of 1000) |#####                | Elapsed Time: 0:00:02 ETA:   0:00:07\n",
      " 28% (286 of 1000) |######               | Elapsed Time: 0:00:02 ETA:   0:00:07\n",
      " 29% (292 of 1000) |######               | Elapsed Time: 0:00:02 ETA:   0:00:07\n",
      " 30% (303 of 1000) |######               | Elapsed Time: 0:00:03 ETA:   0:00:07\n",
      " 31% (313 of 1000) |######               | Elapsed Time: 0:00:03 ETA:   0:00:06\n",
      " 32% (323 of 1000) |######               | Elapsed Time: 0:00:03 ETA:   0:00:06\n",
      " 33% (330 of 1000) |######               | Elapsed Time: 0:00:03 ETA:   0:00:06\n",
      " 34% (340 of 1000) |#######              | Elapsed Time: 0:00:03 ETA:   0:00:06\n",
      " 35% (350 of 1000) |#######              | Elapsed Time: 0:00:03 ETA:   0:00:06\n",
      " 35% (355 of 1000) |#######              | Elapsed Time: 0:00:03 ETA:   0:00:06\n",
      " 36% (365 of 1000) |#######              | Elapsed Time: 0:00:03 ETA:   0:00:06\n",
      " 37% (375 of 1000) |#######              | Elapsed Time: 0:00:03 ETA:   0:00:06\n",
      " 38% (380 of 1000) |#######              | Elapsed Time: 0:00:03 ETA:   0:00:06\n",
      " 39% (390 of 1000) |########             | Elapsed Time: 0:00:03 ETA:   0:00:06\n",
      " 40% (400 of 1000) |########             | Elapsed Time: 0:00:04 ETA:   0:00:06\n",
      " 40% (406 of 1000) |########             | Elapsed Time: 0:00:04 ETA:   0:00:05\n",
      " 41% (416 of 1000) |########             | Elapsed Time: 0:00:04 ETA:   0:00:05\n",
      " 42% (426 of 1000) |########             | Elapsed Time: 0:00:04 ETA:   0:00:05\n",
      " 43% (431 of 1000) |#########            | Elapsed Time: 0:00:04 ETA:   0:00:05\n",
      " 44% (441 of 1000) |#########            | Elapsed Time: 0:00:04 ETA:   0:00:05\n",
      " 45% (451 of 1000) |#########            | Elapsed Time: 0:00:04 ETA:   0:00:05\n",
      " 45% (456 of 1000) |#########            | Elapsed Time: 0:00:04 ETA:   0:00:05\n",
      " 46% (466 of 1000) |#########            | Elapsed Time: 0:00:04 ETA:   0:00:05\n",
      " 47% (476 of 1000) |#########            | Elapsed Time: 0:00:04 ETA:   0:00:05\n",
      " 48% (482 of 1000) |##########           | Elapsed Time: 0:00:04 ETA:   0:00:05\n",
      " 49% (492 of 1000) |##########           | Elapsed Time: 0:00:04 ETA:   0:00:05\n",
      " 50% (502 of 1000) |##########           | Elapsed Time: 0:00:05 ETA:   0:00:05\n",
      " 50% (507 of 1000) |##########           | Elapsed Time: 0:00:05 ETA:   0:00:04\n",
      " 51% (518 of 1000) |##########           | Elapsed Time: 0:00:05 ETA:   0:00:04\n",
      " 52% (528 of 1000) |###########          | Elapsed Time: 0:00:05 ETA:   0:00:04\n",
      " 53% (538 of 1000) |###########          | Elapsed Time: 0:00:05 ETA:   0:00:04\n",
      " 54% (545 of 1000) |###########          | Elapsed Time: 0:00:05 ETA:   0:00:04\n",
      " 55% (556 of 1000) |###########          | Elapsed Time: 0:00:05 ETA:   0:00:04\n",
      " 56% (566 of 1000) |###########          | Elapsed Time: 0:00:05 ETA:   0:00:04\n",
      " 57% (576 of 1000) |############         | Elapsed Time: 0:00:05 ETA:   0:00:04\n",
      " 58% (583 of 1000) |############         | Elapsed Time: 0:00:05 ETA:   0:00:04\n",
      " 59% (594 of 1000) |############         | Elapsed Time: 0:00:05 ETA:   0:00:04\n",
      " 60% (604 of 1000) |############         | Elapsed Time: 0:00:06 ETA:   0:00:03\n",
      " 61% (614 of 1000) |############         | Elapsed Time: 0:00:06 ETA:   0:00:03\n",
      " 62% (621 of 1000) |#############        | Elapsed Time: 0:00:06 ETA:   0:00:03\n",
      " 63% (632 of 1000) |#############        | Elapsed Time: 0:00:06 ETA:   0:00:03\n",
      " 64% (643 of 1000) |#############        | Elapsed Time: 0:00:06 ETA:   0:00:03\n",
      " 65% (653 of 1000) |#############        | Elapsed Time: 0:00:06 ETA:   0:00:03\n",
      " 65% (659 of 1000) |#############        | Elapsed Time: 0:00:06 ETA:   0:00:03\n",
      " 66% (669 of 1000) |##############       | Elapsed Time: 0:00:06 ETA:   0:00:03\n",
      " 67% (679 of 1000) |##############       | Elapsed Time: 0:00:06 ETA:   0:00:03\n",
      " 68% (684 of 1000) |##############       | Elapsed Time: 0:00:06 ETA:   0:00:03\n",
      " 69% (694 of 1000) |##############       | Elapsed Time: 0:00:06 ETA:   0:00:03\n",
      " 70% (704 of 1000) |##############       | Elapsed Time: 0:00:07 ETA:   0:00:02\n",
      " 70% (709 of 1000) |##############       | Elapsed Time: 0:00:07 ETA:   0:00:02\n",
      " 71% (719 of 1000) |###############      | Elapsed Time: 0:00:07 ETA:   0:00:02\n",
      " 72% (729 of 1000) |###############      | Elapsed Time: 0:00:07 ETA:   0:00:02\n",
      " 73% (735 of 1000) |###############      | Elapsed Time: 0:00:07 ETA:   0:00:02\n",
      " 74% (745 of 1000) |###############      | Elapsed Time: 0:00:07 ETA:   0:00:02\n",
      " 75% (756 of 1000) |###############      | Elapsed Time: 0:00:07 ETA:   0:00:02\n",
      " 76% (766 of 1000) |################     | Elapsed Time: 0:00:07 ETA:   0:00:02\n",
      " 77% (773 of 1000) |################     | Elapsed Time: 0:00:07 ETA:   0:00:02\n",
      " 78% (783 of 1000) |################     | Elapsed Time: 0:00:07 ETA:   0:00:02\n",
      " 79% (793 of 1000) |################     | Elapsed Time: 0:00:07 ETA:   0:00:02\n",
      " 79% (798 of 1000) |################     | Elapsed Time: 0:00:08 ETA:   0:00:02\n",
      " 80% (808 of 1000) |################     | Elapsed Time: 0:00:08 ETA:   0:00:01\n",
      " 81% (818 of 1000) |#################    | Elapsed Time: 0:00:08 ETA:   0:00:01\n",
      " 82% (823 of 1000) |#################    | Elapsed Time: 0:00:08 ETA:   0:00:01\n",
      " 83% (833 of 1000) |#################    | Elapsed Time: 0:00:08 ETA:   0:00:01\n",
      " 84% (843 of 1000) |#################    | Elapsed Time: 0:00:08 ETA:   0:00:01\n",
      " 84% (849 of 1000) |#################    | Elapsed Time: 0:00:08 ETA:   0:00:01\n",
      " 85% (859 of 1000) |##################   | Elapsed Time: 0:00:08 ETA:   0:00:01\n",
      " 86% (869 of 1000) |##################   | Elapsed Time: 0:00:08 ETA:   0:00:01\n",
      " 87% (874 of 1000) |##################   | Elapsed Time: 0:00:08 ETA:   0:00:01\n",
      " 88% (884 of 1000) |##################   | Elapsed Time: 0:00:08 ETA:   0:00:01\n",
      " 89% (894 of 1000) |##################   | Elapsed Time: 0:00:09 ETA:   0:00:01\n",
      " 89% (899 of 1000) |##################   | Elapsed Time: 0:00:09 ETA:   0:00:01\n",
      " 90% (909 of 1000) |###################  | Elapsed Time: 0:00:09 ETA:   0:00:00\n",
      " 91% (919 of 1000) |###################  | Elapsed Time: 0:00:09 ETA:   0:00:00\n",
      " 92% (925 of 1000) |###################  | Elapsed Time: 0:00:09 ETA:   0:00:00\n",
      " 93% (935 of 1000) |###################  | Elapsed Time: 0:00:09 ETA:   0:00:00\n",
      " 94% (945 of 1000) |###################  | Elapsed Time: 0:00:09 ETA:   0:00:00\n",
      " 95% (950 of 1000) |###################  | Elapsed Time: 0:00:09 ETA:   0:00:00\n",
      " 96% (960 of 1000) |#################### | Elapsed Time: 0:00:09 ETA:   0:00:00\n",
      " 97% (970 of 1000) |#################### | Elapsed Time: 0:00:09 ETA:   0:00:00\n",
      " 97% (975 of 1000) |#################### | Elapsed Time: 0:00:09 ETA:   0:00:00\n",
      " 98% (985 of 1000) |#################### | Elapsed Time: 0:00:09 ETA:   0:00:00\n",
      " 99% (995 of 1000) |#################### | Elapsed Time: 0:00:10 ETA:   0:00:00\n",
      "100% (1000 of 1000) |####################| Elapsed Time: 0:00:10 Time:  0:00:10\n"
     ]
    }
   ],
   "source": [
    "my_time_series_dataset = []\n",
    "for x in progressbar.progressbar(sorted(glob.glob(\"../../../Data/london_clean/*.csv\"))[:2000]):\n",
    "    df = pd.read_csv(f'{x}')\n",
    "    df[\"DateTime\"] = pd.to_datetime(df['DateTime'])\n",
    "    #df = df.groupby(pd.Grouper(key='DateTime', freq='1D')).max(\"KWHhh\").round(3).reset_index()\n",
    "    series = TimeSeries.from_dataframe(df, time_col='DateTime', value_cols='KWHhh').astype(np.float32)\n",
    "    my_time_series_dataset.append(series)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sets\n",
    "training_sets = []\n",
    "validation_sets = []\n",
    "for x in my_time_series_dataset:\n",
    "    train, val = x.split_after(0.90)\n",
    "    training_sets.append(train)\n",
    "    validation_sets.append(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "We create a N-Beats model that utilizes the GPU, Weights, Biases logger and early stopping callback.\n",
    "\n",
    "## Early stopping\n",
    "\n",
    "An early stopping callback is used to stop the training if the validation loss does not improve after a certain number of epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=0.01,\n",
    "    patience=4,\n",
    "    verbose=True,\n",
    "    mode=\"min\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoders = {\n",
    "    # \"datetime_attribute\": {\"future\": [\"DateTime\"], \"past\": [\"DateTime\"]},\n",
    "    \"position\": {\"past\": [\"absolute\"], \"future\": [\"relative\"]},\n",
    "    \"transformer\": Scaler(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_logger = WandbLogger(project=\"Digital-Energy\", name=MODEL_NAME, log_model=True)\n",
    "\n",
    "\n",
    "# input chunk = The length of the input sequence fed to the model\n",
    "# output chunk = The length of the output sequence predicted by the model\n",
    "\n",
    "model_nbeats = NBEATSModel(\n",
    "    input_chunk_length=96*3,\n",
    "    output_chunk_length=96,\n",
    "    generic_architecture=False,\n",
    "    #num_stacks=10,\n",
    "    num_blocks=3,\n",
    "    num_layers=5,\n",
    "    layer_widths=512,\n",
    "    n_epochs=15,\n",
    "    nr_epochs_val_period=1,\n",
    "    batch_size=2048,\n",
    "    work_dir=\"../../../Models\",\n",
    "    save_checkpoints=True,\n",
    "    model_name=MODEL_NAME,\n",
    "    pl_trainer_kwargs={\n",
    "    \"enable_progress_bar\": True,\n",
    "    \"enable_model_summary\": True,\n",
    "    \"accelerator\": \"gpu\",\n",
    "    \"devices\": [1],\n",
    "    \"logger\": wandb_logger,\n",
    "    \"callbacks\": [early_stop_callback]\n",
    "    },\n",
    "    # loss_fn=torch.nn.CrossEntropyLoss() # custom loss function\n",
    "    # optimizer_cls=torch.optim.Adam,\n",
    "    # add_encoders=encoders,\n",
    "    log_tensorboard=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wandb_logger.watch(model_nbeats) # sadly this feature does not work for Darts models\n",
    "model_nbeats.fit(series=training_sets, val_series=validation_sets, num_loader_workers=AVAILABLE_CPUS, max_samples_per_ts=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START = 3000\n",
    "for i, x in enumerate(sorted(glob.glob(\"../../../Data/london_clean/*.csv\"))[START:START+10]):\n",
    "\n",
    "    df = pd.read_csv(x)\n",
    "    df[\"DateTime\"] = pd.to_datetime(df['DateTime'])\n",
    "    series = TimeSeries.from_dataframe(df, value_cols=['KWHhh'], time_col=\"DateTime\", fill_missing_dates=True, freq=\"30min\").astype(np.float32)\n",
    "    series = series[-600:]\n",
    "\n",
    "\n",
    "    pred_series = model_nbeats.historical_forecasts(\n",
    "        series,\n",
    "        forecast_horizon=5,\n",
    "        stride=1,\n",
    "        retrain=False,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    print(f\"rmse: {rmse(series, pred_series)}.\")\n",
    "    print(f\"R2 score: {r2_score(series, pred_series)}.\")\n",
    "\n",
    "    helper.display_forecast(pred_series, series, \"1 day\", save=True, fig_name=f\"{i}-test\", model_name=f\"{MODEL_NAME}\", fig_size=(20,10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading checkpoints of the model\n",
    "\n",
    "loading the best checkpoint of the model. To compare the results of the model with the previous one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "model_nbeats = NBEATSModel.load_from_checkpoint(work_dir=\"../../Models/\", model_name=MODEL_NAME, best=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START = 3000\n",
    "for i, x in enumerate(sorted(glob.glob(\"../../Data/london_clean/*.csv\"))[START:START+10]):\n",
    "\n",
    "    df = pd.read_csv(x)\n",
    "    df[\"DateTime\"] = pd.to_datetime(df['DateTime'])\n",
    "    series = TimeSeries.from_dataframe(df, value_cols=['KWHhh'], time_col=\"DateTime\", fill_missing_dates=True, freq=\"30min\").astype(np.float32)\n",
    "    series = series[-600:]\n",
    "\n",
    "\n",
    "    pred_series = model_nbeats.predict(\n",
    "        1,\n",
    "        series,\n",
    "    )\n",
    "\n",
    "    print(f\"rmse: {rmse(series, pred_series)}.\")\n",
    "    print(f\"R2 score: {r2_score(series, pred_series)}.\")\n",
    "\n",
    "    helper.display_forecast(pred_series, series, \"1 day\", save=True, fig_name=f\"{i}-test\", model_name=f\"{MODEL_NAME}\", fig_size=(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.display_forecast(pred_series, series, \"1 day\", save=False, fig_name=f\"test\", model_name=f\"{MODEL_NAME}\", fig_size=(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
