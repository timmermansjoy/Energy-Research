{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"n_beats_biggysmall\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from multiprocessing.dummy import freeze_support\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = 'n-beats.ipynb'\n",
    "os.environ['WANDB_API_KEY'] = os.getenv('WANDB_API_KEY')\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "from darts import TimeSeries\n",
    "from darts.models import NBEATSModel\n",
    "from darts.dataprocessing.transformers import Scaler, MissingValuesFiller\n",
    "from darts.metrics import mape, r2_score, rmse\n",
    "\n",
    "from darts import TimeSeries\n",
    "\n",
    "from darts.datasets import EnergyDataset\n",
    "\n",
    "import helper\n",
    "import glob\n",
    "\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "wandb_logger = WandbLogger(project=\"Digital-Energy\", name=MODEL_NAME, log_model=True)\n",
    "\n",
    "\n",
    "AVAILABLE_GPUS = torch.cuda.device_count()\n",
    "AVAILABLE_CPUS = os.cpu_count()\n",
    "\n",
    "print(f\"Available GPUs: {AVAILABLE_GPUS}\")\n",
    "print(f\"Available CPUs: {AVAILABLE_CPUS}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "Taking the first 300 households from the London Dataset and converting them to a Darts TimeSeries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_time_series_dataset = []\n",
    "for x in sorted(glob.glob(\"../../Data/london_clean/*.csv\"))[:1000]:\n",
    "    df = pd.read_csv(f'{x}')\n",
    "    df[\"DateTime\"] = pd.to_datetime(df['DateTime'])\n",
    "    #df = df.groupby(pd.Grouper(key='DateTime', freq='1D')).max(\"KWHhh\").round(3).reset_index()\n",
    "    series = TimeSeries.from_dataframe(df, time_col='DateTime', value_cols='KWHhh').astype(np.float32)\n",
    "    my_time_series_dataset.append(series)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sets\n",
    "training_sets = []\n",
    "validation_sets = []\n",
    "for x in my_time_series_dataset:\n",
    "    train, val = series.split_after(0.90)\n",
    "    training_sets.append(train)\n",
    "    validation_sets.append(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "We create a N-Beats model that utilizes the GPU, Weights, Biases logger and early stopping callback.\n",
    "\n",
    "## Early stopping\n",
    "\n",
    "An early stopping callback is used to stop the training if the validation loss does not improve after a certain number of epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=0.01,\n",
    "    patience=10,\n",
    "    verbose=True,\n",
    "    mode=\"min\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoders = {\n",
    "    # \"datetime_attribute\": {\"future\": [\"DateTime\"], \"past\": [\"DateTime\"]},\n",
    "    \"position\": {\"past\": [\"absolute\"], \"future\": [\"relative\"]},\n",
    "    \"transformer\": Scaler(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input chunk = The length of the input sequence fed to the model\n",
    "# output chunk = The length of the output sequence predicted by the model\n",
    "model_nbeats = NBEATSModel(\n",
    "    input_chunk_length=96,\n",
    "    output_chunk_length=1,\n",
    "    generic_architecture=False,\n",
    "    #num_stacks=10,\n",
    "    num_blocks=3,\n",
    "    num_layers=5,\n",
    "    layer_widths=512,\n",
    "    n_epochs=40,\n",
    "    nr_epochs_val_period=1,\n",
    "    batch_size=2048,\n",
    "    work_dir=\"../../Models\",\n",
    "    save_checkpoints=True,\n",
    "    model_name=MODEL_NAME,\n",
    "    pl_trainer_kwargs={\n",
    "    \"enable_progress_bar\": True,\n",
    "    \"enable_model_summary\": True,\n",
    "    \"accelerator\": \"gpu\",\n",
    "    \"devices\": 1,\n",
    "    \"logger\": wandb_logger,\n",
    "    \"callbacks\": [early_stop_callback]\n",
    "    },\n",
    "    #loss_fn=torch.nn.CrossEntropyLoss() custom loss function\n",
    "    # optimizer_cls=torch.optim.Adam,\n",
    "    log_tensorboard=True,\n",
    "    add_encoders=encoders,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wandb_logger.watch(model_nbeats) # sadly this feature does not work for Darts models\n",
    "model_nbeats.fit(series=training_sets, val_series=validation_sets, num_loader_workers=AVAILABLE_CPUS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START = 3000\n",
    "for i, x in enumerate(glob.glob(\"../../Data/london_clean/*.csv\")[START:START+10]):\n",
    "\n",
    "    df = pd.read_csv(x)\n",
    "    df[\"DateTime\"] = pd.to_datetime(df['DateTime'])\n",
    "    series = TimeSeries.from_dataframe(df, value_cols=['KWHhh'], time_col=\"DateTime\", fill_missing_dates=True, freq=\"30min\").astype(np.float32)\n",
    "    series = series[-500:]\n",
    "\n",
    "\n",
    "    pred_series = model_nbeats.historical_forecasts(\n",
    "        series,\n",
    "        forecast_horizon=1,\n",
    "        stride=1,\n",
    "        retrain=False,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    print(f\"rmse: {rmse(series, pred_series)}.\")\n",
    "    print(f\"R2 score: {r2_score(series, pred_series)}.\")\n",
    "\n",
    "    helper.display_forecast(pred_series, series, \"1 day\", save=True, fig_name=f\"{i}-test\", model_name=f\"{MODEL_NAME}\", fig_size=(20,10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading checkpoints of the model\n",
    "\n",
    "loading the best checkpoint of the model. To compare the results of the model with the previous one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "model_nbeats = NBEATSModel.load_from_checkpoint(work_dir=\"../../Models/\", model_name=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.display_forecast(pred_series, series, \"1 day\", save=False, fig_name=f\"test\", model_name=f\"{MODEL_NAME}\", fig_size=(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
