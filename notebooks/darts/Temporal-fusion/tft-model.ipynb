{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"tft-nb-4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-10 09:49:12 apex.transformer.tensor_parallel WARNING: `fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
      "2022-05-10 09:49:12 root WARNING: Bagua cannot detect bundled NCCL library, Bagua will try to use system NCCL instead. If you encounter any error, please run `import bagua_core; bagua_core.install_deps()` or the `bagua_install_deps.py` script to install bundled libraries.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPUs: 2\n",
      "Available CPUs: 32\n",
      "Available CPUs: 32\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from multiprocessing.dummy import freeze_support\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = 'pytorch_stats_own_data.ipynb'\n",
    "os.environ['WANDB_API_KEY'] = os.getenv('WANDB_API_KEY')\n",
    "\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "from darts import TimeSeries\n",
    "from darts.models import TFTModel\n",
    "from darts.dataprocessing.transformers import Scaler, MissingValuesFiller\n",
    "from darts.utils.likelihood_models import QuantileRegression\n",
    "from darts.metrics import mape, r2_score, rmse, mse\n",
    "from darts import TimeSeries\n",
    "\n",
    "import helper\n",
    "import glob\n",
    "\n",
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.strategies import DDPStrategy, ddp2\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from tqdm.contrib.concurrent import process_map\n",
    "import tqdm\n",
    "\n",
    "\n",
    "AVAILABLE_GPUS = torch.cuda.device_count()\n",
    "AVAILABLE_CPUS = os.cpu_count()\n",
    "TRAINING_DATA_PATH = \"../../../Data/london_clean/*.csv\"\n",
    "\n",
    "print(f\"Available GPUs: {AVAILABLE_GPUS}\")\n",
    "print(f\"Available CPUs: {AVAILABLE_CPUS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "Taking the first 300 households from the London Dataset and converting them to a Darts TimeSeries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74c22cfe287045aca91c1bbe39cfa24c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def reader(x):\n",
    "    return TimeSeries.from_csv(x, time_col='DateTime', value_cols='KWHhh',\n",
    "                            fill_missing_dates=True, fillna_value=True, freq=\"30min\").astype(np.float32)\n",
    "\n",
    "\n",
    "def splitter():\n",
    "    file_list = sorted(glob.glob(\"../../../Data/london_clean/*.csv\"))[:1000]\n",
    "    if file_list == []:\n",
    "        raise Exception(\"No files found\")\n",
    "    return process_map(reader, file_list, chunksize=5)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    freeze_support()\n",
    "    my_time_series_dataset = splitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sets\n",
    "training_sets = []\n",
    "validation_sets = []\n",
    "for x in my_time_series_dataset:\n",
    "    train, val = x.split_after(0.85)\n",
    "    training_sets.append(train)\n",
    "    validation_sets.append(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "We create a N-Beats model that utilizes the GPU, Weights, Biases logger and early stopping callback.\n",
    "\n",
    "## Early stopping\n",
    "\n",
    "An early stopping callback is used to stop the training if the validation loss does not improve after a certain number of epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=0.01,\n",
    "    patience=10,\n",
    "    verbose=True,\n",
    "    mode=\"min\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoders = {\n",
    "    # \"datetime_attribute\": {\"future\": [\"DateTime\"], \"past\": [\"DateTime\"]},\n",
    "    \"position\": {\"past\": [\"absolute\"], \"future\": [\"relative\"]},\n",
    "    \"transformer\": Scaler(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find pytorch_stats_own_data.ipynb.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtimmermansjoy\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/notebooks/darts/Temporal-fusion/wandb/run-20220510_094919-12ywakxf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/timmermansjoy/digital-energy/runs/12ywakxf\" target=\"_blank\">vivid-bird-319</a></strong> to <a href=\"https://wandb.ai/timmermansjoy/digital-energy\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/torch/random.py:99: UserWarning: CUDA reports that you have 2 available devices, and you have used fork_rng without explicitly specifying which devices are being used. For safety, we initialize *every* CUDA device by default, which can be quite slow if you have a lot of GPUs.  If you know that you are only making use of a few CUDA devices, set the environment variable CUDA_VISIBLE_DEVICES or the 'devices' keyword argument of fork_rng with the set of devices you are actually using.  For example, if you are using CPU only, set CUDA_VISIBLE_DEVICES= or devices=[]; if you are using GPU 0 only, set CUDA_VISIBLE_DEVICES=0 or devices=[0].  To initialize all devices and suppress this warning, set the 'devices' keyword argument to `range(torch.cuda.device_count())`.\n",
      "  warnings.warn(\n",
      "Exception in thread Thread-16:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/wandb/apis/normalize.py\", line 22, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/wandb/sdk/internal/internal_api.py\", line 1434, in upload_urls\n",
      "    raise CommError(f\"Run does not exist {entity}/{project}/{run_id}.\")\n",
      "wandb.errors.CommError: Run does not exist timmermansjoy/digital-energy/12ywakxf.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.9/threading.py\", line 973, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/wandb/filesync/upload_job.py\", line 56, in run\n",
      "    success = self.push()\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/wandb/filesync/upload_job.py\", line 107, in push\n",
      "    _, upload_headers, result = self._api.upload_urls(project, [self.save_name])\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/wandb/apis/normalize.py\", line 58, in wrapper\n",
      "    raise CommError(message, err).with_traceback(sys.exc_info()[2])\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/wandb/apis/normalize.py\", line 22, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/wandb/sdk/internal/internal_api.py\", line 1434, in upload_urls\n",
      "    raise CommError(f\"Run does not exist {entity}/{project}/{run_id}.\")\n",
      "wandb.errors.CommError: Run does not exist timmermansjoy/digital-energy/12ywakxf.\n"
     ]
    }
   ],
   "source": [
    "wandb_logger = WandbLogger(project=\"Digital-Energy\", log_model=True)\n",
    "\n",
    "\n",
    "# input chunk = The length of the input sequence fed to the model\n",
    "# output chunk = The length of the output sequence predicted by the model\n",
    "\n",
    "\n",
    "# default quantiles for QuantileRegression\n",
    "quantiles = [\n",
    "    0.01,\n",
    "    0.05,\n",
    "    0.1,\n",
    "    0.15,\n",
    "    0.2,\n",
    "    0.25,\n",
    "    0.3,\n",
    "    0.4,\n",
    "    0.5,\n",
    "    0.6,\n",
    "    0.7,\n",
    "    0.75,\n",
    "    0.8,\n",
    "    0.85,\n",
    "    0.9,\n",
    "    0.95,\n",
    "    0.99,\n",
    "]\n",
    "\n",
    "input_chunk_length = 96\n",
    "forecast_horizon = 1\n",
    "\n",
    "model = TFTModel(\n",
    "    input_chunk_length=input_chunk_length,\n",
    "    output_chunk_length=forecast_horizon,\n",
    "    hidden_size=89,\n",
    "    lstm_layers=7,\n",
    "    num_attention_heads=6,\n",
    "    dropout=0.04,\n",
    "    batch_size=2048,\n",
    "    n_epochs=100,\n",
    "    add_relative_index=True,\n",
    "    add_encoders=encoders,\n",
    "    work_dir=\"../../../Models\",\n",
    "    save_checkpoints=False,\n",
    "    pl_trainer_kwargs={\n",
    "    \"enable_progress_bar\": True,\n",
    "    \"enable_model_summary\": True,\n",
    "    \"accelerator\": \"gpu\",\n",
    "    \"devices\": [1],\n",
    "    \"logger\": wandb_logger,\n",
    "    \"callbacks\": [early_stop_callback]\n",
    "    },\n",
    "    likelihood=QuantileRegression(\n",
    "        quantiles=quantiles\n",
    "    ),  # QuantileRegression is set per default\n",
    "    random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-05-10 09:49:33,063] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 500000 samples.\n",
      "[2022-05-10 09:49:33,063] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 500000 samples.\n",
      "2022-05-10 09:49:33 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 500000 samples.\n",
      "[2022-05-10 09:49:33,084] INFO | darts.models.forecasting.torch_forecasting_model | Time series values are 32-bits; casting model to float32.\n",
      "[2022-05-10 09:49:33,084] INFO | darts.models.forecasting.torch_forecasting_model | Time series values are 32-bits; casting model to float32.\n",
      "2022-05-10 09:49:33 darts.models.forecasting.torch_forecasting_model INFO: Time series values are 32-bits; casting model to float32.\n",
      "2022-05-10 09:49:33 pytorch_lightning.utilities.rank_zero INFO: GPU available: True, used: True\n",
      "2022-05-10 09:49:33 pytorch_lightning.utilities.rank_zero INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-05-10 09:49:33 pytorch_lightning.utilities.rank_zero INFO: IPU available: False, using: 0 IPUs\n",
      "2022-05-10 09:49:33 pytorch_lightning.utilities.rank_zero INFO: HPU available: False, using: 0 HPUs\n",
      "2022-05-10 09:49:35 pytorch_lightning.accelerators.gpu INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "2022-05-10 09:49:35 pytorch_lightning.callbacks.model_summary INFO: \n",
      "   | Name                              | Type                             | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | static_covariates_vsn             | _VariableSelectionNetwork        | 0     \n",
      "1  | encoder_vsn                       | _VariableSelectionNetwork        | 9.4 K \n",
      "2  | decoder_vsn                       | _VariableSelectionNetwork        | 4.7 K \n",
      "3  | static_context_grn                | _GatedResidualNetwork            | 32.2 K\n",
      "4  | static_context_hidden_encoder_grn | _GatedResidualNetwork            | 32.2 K\n",
      "5  | static_context_cell_encoder_grn   | _GatedResidualNetwork            | 32.2 K\n",
      "6  | static_context_enrichment         | _GatedResidualNetwork            | 32.2 K\n",
      "7  | lstm_encoder                      | LSTM                             | 448 K \n",
      "8  | lstm_decoder                      | LSTM                             | 448 K \n",
      "9  | post_lstm_gan                     | _GateAddNorm                     | 16.2 K\n",
      "10 | static_enrichment_grn             | _GatedResidualNetwork            | 40.1 K\n",
      "11 | multihead_attn                    | _InterpretableMultiHeadAttention | 17.6 K\n",
      "12 | post_attn_gan                     | _GateAddNorm                     | 16.2 K\n",
      "13 | positionwise_feedforward_grn      | _GatedResidualNetwork            | 32.2 K\n",
      "14 | pre_output_gan                    | _GateAddNorm                     | 16.2 K\n",
      "15 | output_layer                      | Linear                           | 1.5 K \n",
      "----------------------------------------------------------------------------------------\n",
      "1.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 M     Total params\n",
      "4.720     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8386cded3d7144b2b3ca44e03da5e083",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "382e7a50984f4e53a7be80cbc4d99e34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e16ac866ed24c9db689dd4ff1497681",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-05-10 09:54:30,198] ERROR | main_logger | ValueError: The dataset contains some time series that are too short to contain `max(self.input_chunk_length, self.shift + self.output_chunk_length)` (551-th series)\n",
      "[2022-05-10 09:54:30,198] ERROR | main_logger | ValueError: The dataset contains some time series that are too short to contain `max(self.input_chunk_length, self.shift + self.output_chunk_length)` (551-th series)\n",
      "[2022-05-10 09:54:30,198] ERROR | main_logger | ValueError: The dataset contains some time series that are too short to contain `max(self.input_chunk_length, self.shift + self.output_chunk_length)` (551-th series)\n",
      "[2022-05-10 09:54:30,198] ERROR | main_logger | ValueError: The dataset contains some time series that are too short to contain `max(self.input_chunk_length, self.shift + self.output_chunk_length)` (551-th series)\n",
      "[2022-05-10 09:54:30,198] ERROR | main_logger | ValueError: The dataset contains some time series that are too short to contain `max(self.input_chunk_length, self.shift + self.output_chunk_length)` (551-th series)\n",
      "2022-05-10 09:54:30 main_logger ERROR: ValueError: The dataset contains some time series that are too short to contain `max(self.input_chunk_length, self.shift + self.output_chunk_length)` (551-th series)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Caught ValueError in DataLoader worker process 6.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.9/dist-packages/darts/utils/data/sequential_dataset.py\", line 327, in __getitem__\n    past_target, past_covariate, future_target = self.ds_past[idx]\n  File \"/usr/local/lib/python3.9/dist-packages/darts/utils/data/shifted_dataset.py\", line 530, in __getitem__\n    raise_if_not(\n  File \"/usr/local/lib/python3.9/dist-packages/darts/logging.py\", line 84, in raise_if_not\n    raise ValueError(message)\nValueError: The dataset contains some time series that are too short to contain `max(self.input_chunk_length, self.shift + self.output_chunk_length)` (551-th series)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/workspace/notebooks/darts/Temporal-fusion/tft-model.ipynb Cell 11'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f686f6d652f6a6f792f456e657267792d5265736561726368222c226c6f63616c446f636b6572223a66616c73652c2273657474696e6773223a7b22686f7374223a227373683a2f2f31302e3132352e39332e37227d7d/workspace/notebooks/darts/Temporal-fusion/tft-model.ipynb#ch0000010vscode-remote?line=0'>1</a>\u001b[0m \u001b[39m#wandb_logger.watch(model_nbeats) # sadly this feature does not work for Darts models\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f686f6d652f6a6f792f456e657267792d5265736561726368222c226c6f63616c446f636b6572223a66616c73652c2273657474696e6773223a7b22686f7374223a227373683a2f2f31302e3132352e39332e37227d7d/workspace/notebooks/darts/Temporal-fusion/tft-model.ipynb#ch0000010vscode-remote?line=1'>2</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(series\u001b[39m=\u001b[39;49mtraining_sets, val_series\u001b[39m=\u001b[39;49mvalidation_sets, num_loader_workers\u001b[39m=\u001b[39;49mAVAILABLE_CPUS\u001b[39m/\u001b[39;49m\u001b[39m/\u001b[39;49m\u001b[39m2\u001b[39;49m, max_samples_per_ts\u001b[39m=\u001b[39;49m\u001b[39m500\u001b[39;49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/darts/utils/torch.py:70\u001b[0m, in \u001b[0;36mrandom_method.<locals>.decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.9/dist-packages/darts/utils/torch.py?line=67'>68</a>\u001b[0m \u001b[39mwith\u001b[39;00m fork_rng():\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.9/dist-packages/darts/utils/torch.py?line=68'>69</a>\u001b[0m     manual_seed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_random_instance\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, high\u001b[39m=\u001b[39mMAX_TORCH_SEED_VALUE))\n\u001b[0;32m---> <a href='file:///usr/local/lib/python3.9/dist-packages/darts/utils/torch.py?line=69'>70</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m decorated(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/darts/models/forecasting/torch_forecasting_model.py:771\u001b[0m, in \u001b[0;36mTorchForecastingModel.fit\u001b[0;34m(self, series, past_covariates, future_covariates, val_series, val_past_covariates, val_future_covariates, trainer, verbose, epochs, max_samples_per_ts, num_loader_workers)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/darts/models/forecasting/torch_forecasting_model.py?line=766'>767</a>\u001b[0m     val_dataset \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/darts/models/forecasting/torch_forecasting_model.py?line=768'>769</a>\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTrain dataset contains \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(train_dataset)\u001b[39m}\u001b[39;00m\u001b[39m samples.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> <a href='file:///usr/local/lib/python3.9/dist-packages/darts/models/forecasting/torch_forecasting_model.py?line=770'>771</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_from_dataset(\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/darts/models/forecasting/torch_forecasting_model.py?line=771'>772</a>\u001b[0m     train_dataset, val_dataset, trainer, verbose, epochs, num_loader_workers\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/darts/models/forecasting/torch_forecasting_model.py?line=772'>773</a>\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/darts/utils/torch.py:70\u001b[0m, in \u001b[0;36mrandom_method.<locals>.decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.9/dist-packages/darts/utils/torch.py?line=67'>68</a>\u001b[0m \u001b[39mwith\u001b[39;00m fork_rng():\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.9/dist-packages/darts/utils/torch.py?line=68'>69</a>\u001b[0m     manual_seed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_random_instance\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, high\u001b[39m=\u001b[39mMAX_TORCH_SEED_VALUE))\n\u001b[0;32m---> <a href='file:///usr/local/lib/python3.9/dist-packages/darts/utils/torch.py?line=69'>70</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m decorated(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/darts/models/forecasting/torch_forecasting_model.py:930\u001b[0m, in \u001b[0;36mTorchForecastingModel.fit_from_dataset\u001b[0;34m(self, train_dataset, val_dataset, trainer, verbose, epochs, num_loader_workers)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/darts/models/forecasting/torch_forecasting_model.py?line=920'>921</a>\u001b[0m     logger\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/darts/models/forecasting/torch_forecasting_model.py?line=921'>922</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAttempting to retrain the model without resuming from a checkpoint. This is currently \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/darts/models/forecasting/torch_forecasting_model.py?line=922'>923</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdiscouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/darts/models/forecasting/torch_forecasting_model.py?line=925'>926</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`new_epochs` is the sum of (epochs already trained + some additional epochs).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/darts/models/forecasting/torch_forecasting_model.py?line=926'>927</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/darts/models/forecasting/torch_forecasting_model.py?line=928'>929</a>\u001b[0m \u001b[39m# Train model\u001b[39;00m\n\u001b[0;32m--> <a href='file:///usr/local/lib/python3.9/dist-packages/darts/models/forecasting/torch_forecasting_model.py?line=929'>930</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train(train_loader, val_loader)\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/darts/models/forecasting/torch_forecasting_model.py?line=930'>931</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/darts/models/forecasting/torch_forecasting_model.py:952\u001b[0m, in \u001b[0;36mTorchForecastingModel._train\u001b[0;34m(self, train_loader, val_loader)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/darts/models/forecasting/torch_forecasting_model.py?line=948'>949</a>\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload_ckpt_path\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/darts/models/forecasting/torch_forecasting_model.py?line=949'>950</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload_ckpt_path \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> <a href='file:///usr/local/lib/python3.9/dist-packages/darts/models/forecasting/torch_forecasting_model.py?line=951'>952</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/darts/models/forecasting/torch_forecasting_model.py?line=952'>953</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel,\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/darts/models/forecasting/torch_forecasting_model.py?line=953'>954</a>\u001b[0m     train_dataloaders\u001b[39m=\u001b[39;49mtrain_loader,\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/darts/models/forecasting/torch_forecasting_model.py?line=954'>955</a>\u001b[0m     val_dataloaders\u001b[39m=\u001b[39;49mval_loader,\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/darts/models/forecasting/torch_forecasting_model.py?line=955'>956</a>\u001b[0m     ckpt_path\u001b[39m=\u001b[39;49mckpt_path,\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/darts/models/forecasting/torch_forecasting_model.py?line=956'>957</a>\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py:771\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py?line=751'>752</a>\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py?line=752'>753</a>\u001b[0m \u001b[39mRuns the full optimization routine.\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py?line=753'>754</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py?line=767'>768</a>\u001b[0m \u001b[39m    datamodule: An instance of :class:`~pytorch_lightning.core.datamodule.LightningDataModule`.\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py?line=768'>769</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py?line=769'>770</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m model\n\u001b[0;32m--> <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py?line=770'>771</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py?line=771'>772</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py?line=772'>773</a>\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py:724\u001b[0m, in \u001b[0;36mTrainer._call_and_handle_interrupt\u001b[0;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py?line=721'>722</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py?line=722'>723</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py?line=723'>724</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py?line=724'>725</a>\u001b[0m \u001b[39m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py?line=725'>726</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m \u001b[39mas\u001b[39;00m exception:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py:812\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py?line=807'>808</a>\u001b[0m ckpt_path \u001b[39m=\u001b[39m ckpt_path \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresume_from_checkpoint\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py?line=808'>809</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__set_ckpt_path(\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py?line=809'>810</a>\u001b[0m     ckpt_path, model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py?line=810'>811</a>\u001b[0m )\n\u001b[0;32m--> <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py?line=811'>812</a>\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mckpt_path)\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py?line=813'>814</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py?line=814'>815</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py:1237\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py?line=1232'>1233</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mrestore_training_state()\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py?line=1234'>1235</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mresume_end()\n\u001b[0;32m-> <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py?line=1236'>1237</a>\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py?line=1238'>1239</a>\u001b[0m log\u001b[39m.\u001b[39mdetail(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py?line=1239'>1240</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_teardown()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py:1324\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py?line=1321'>1322</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py?line=1322'>1323</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_predict()\n\u001b[0;32m-> <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py?line=1323'>1324</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_train()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py:1354\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py?line=1351'>1352</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_loop\u001b[39m.\u001b[39mtrainer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py?line=1352'>1353</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py?line=1353'>1354</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_loop\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/base.py:204\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/base.py?line=201'>202</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/base.py?line=202'>203</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/base.py?line=203'>204</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/base.py?line=204'>205</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/base.py?line=205'>206</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/fit_loop.py:269\u001b[0m, in \u001b[0;36mFitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/fit_loop.py?line=264'>265</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_fetcher\u001b[39m.\u001b[39msetup(\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/fit_loop.py?line=265'>266</a>\u001b[0m     dataloader, batch_to_device\u001b[39m=\u001b[39mpartial(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39m_call_strategy_hook, \u001b[39m\"\u001b[39m\u001b[39mbatch_to_device\u001b[39m\u001b[39m\"\u001b[39m, dataloader_idx\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/fit_loop.py?line=266'>267</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/fit_loop.py?line=267'>268</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_epoch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/fit_loop.py?line=268'>269</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_fetcher)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/base.py:205\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/base.py?line=202'>203</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/base.py?line=203'>204</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madvance(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/base.py?line=204'>205</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mon_advance_end()\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/base.py?line=205'>206</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/base.py?line=206'>207</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:255\u001b[0m, in \u001b[0;36mTrainingEpochLoop.on_advance_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py?line=252'>253</a>\u001b[0m \u001b[39mif\u001b[39;00m should_check_val:\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py?line=253'>254</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mvalidating \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py?line=254'>255</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_validation()\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py?line=255'>256</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py?line=257'>258</a>\u001b[0m \u001b[39m# update plateau LR scheduler after metrics are logged\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:309\u001b[0m, in \u001b[0;36mTrainingEpochLoop._run_validation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py?line=305'>306</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mval_loop\u001b[39m.\u001b[39m_reload_evaluation_dataloaders()\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py?line=307'>308</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py?line=308'>309</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mval_loop\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/base.py:204\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/base.py?line=201'>202</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/base.py?line=202'>203</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/base.py?line=203'>204</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/base.py?line=204'>205</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/base.py?line=205'>206</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py:153\u001b[0m, in \u001b[0;36mEvaluationLoop.advance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py?line=150'>151</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_dataloaders \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py?line=151'>152</a>\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mdataloader_idx\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m dataloader_idx\n\u001b[0;32m--> <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py?line=152'>153</a>\u001b[0m dl_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_fetcher, dl_max_batches, kwargs)\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py?line=154'>155</a>\u001b[0m \u001b[39m# store batch level output per dataloader\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py?line=155'>156</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs\u001b[39m.\u001b[39mappend(dl_outputs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/base.py:204\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/base.py?line=201'>202</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/base.py?line=202'>203</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/base.py?line=203'>204</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/base.py?line=204'>205</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/base.py?line=205'>206</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py:111\u001b[0m, in \u001b[0;36mEvaluationEpochLoop.advance\u001b[0;34m(self, data_fetcher, dl_max_batches, kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=108'>109</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(data_fetcher, DataLoaderIterDataFetcher):\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=109'>110</a>\u001b[0m     batch_idx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mcurrent\u001b[39m.\u001b[39mready\n\u001b[0;32m--> <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=110'>111</a>\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(data_fetcher)\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=111'>112</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=112'>113</a>\u001b[0m     batch_idx, batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(data_fetcher)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_lightning/utilities/fetching.py:184\u001b[0m, in \u001b[0;36mAbstractDataFetcher.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/utilities/fetching.py?line=182'>183</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__next__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m--> <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/utilities/fetching.py?line=183'>184</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfetching_function()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_lightning/utilities/fetching.py:259\u001b[0m, in \u001b[0;36mDataFetcher.fetching_function\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/utilities/fetching.py?line=255'>256</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdone:\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/utilities/fetching.py?line=256'>257</a>\u001b[0m     \u001b[39m# this will run only when no pre-fetching was done.\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/utilities/fetching.py?line=257'>258</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/utilities/fetching.py?line=258'>259</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fetch_next_batch(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataloader_iter)\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/utilities/fetching.py?line=259'>260</a>\u001b[0m         \u001b[39m# consume the batch we just fetched\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/utilities/fetching.py?line=260'>261</a>\u001b[0m         batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatches\u001b[39m.\u001b[39mpop(\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_lightning/utilities/fetching.py:273\u001b[0m, in \u001b[0;36mDataFetcher._fetch_next_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/utilities/fetching.py?line=270'>271</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_fetch_next_batch\u001b[39m(\u001b[39mself\u001b[39m, iterator: Iterator) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/utilities/fetching.py?line=271'>272</a>\u001b[0m     start_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_fetch_start()\n\u001b[0;32m--> <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/utilities/fetching.py?line=272'>273</a>\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(iterator)\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/utilities/fetching.py?line=273'>274</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfetched \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/utilities/fetching.py?line=274'>275</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprefetch_batches \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_len:\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/pytorch_lightning/utilities/fetching.py?line=275'>276</a>\u001b[0m         \u001b[39m# when we don't prefetch but the dataloader is sized, we use the length for `done`\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:521\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py?line=518'>519</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py?line=519'>520</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[0;32m--> <a href='file:///usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py?line=520'>521</a>\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py?line=521'>522</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py?line=522'>523</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py?line=523'>524</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py?line=524'>525</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:1203\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py?line=1200'>1201</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py?line=1201'>1202</a>\u001b[0m     \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> <a href='file:///usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py?line=1202'>1203</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_data(data)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:1229\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py?line=1226'>1227</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_try_put_index()\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py?line=1227'>1228</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> <a href='file:///usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py?line=1228'>1229</a>\u001b[0m     data\u001b[39m.\u001b[39;49mreraise()\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py?line=1229'>1230</a>\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/_utils.py:434\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/torch/_utils.py?line=429'>430</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/torch/_utils.py?line=430'>431</a>\u001b[0m     \u001b[39m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/torch/_utils.py?line=431'>432</a>\u001b[0m     \u001b[39m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/dist-packages/torch/_utils.py?line=432'>433</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m--> <a href='file:///usr/local/lib/python3.9/dist-packages/torch/_utils.py?line=433'>434</a>\u001b[0m \u001b[39mraise\u001b[39;00m exception\n",
      "\u001b[0;31mValueError\u001b[0m: Caught ValueError in DataLoader worker process 6.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.9/dist-packages/darts/utils/data/sequential_dataset.py\", line 327, in __getitem__\n    past_target, past_covariate, future_target = self.ds_past[idx]\n  File \"/usr/local/lib/python3.9/dist-packages/darts/utils/data/shifted_dataset.py\", line 530, in __getitem__\n    raise_if_not(\n  File \"/usr/local/lib/python3.9/dist-packages/darts/logging.py\", line 84, in raise_if_not\n    raise ValueError(message)\nValueError: The dataset contains some time series that are too short to contain `max(self.input_chunk_length, self.shift + self.output_chunk_length)` (551-th series)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (ReadTimeout), entering retry loop.\n",
      "wandb: Network error (ReadTimeout), entering retry loop.\n",
      "wandb: Network error (ReadTimeout), entering retry loop.\n"
     ]
    }
   ],
   "source": [
    "#wandb_logger.watch(model_nbeats) # sadly this feature does not work for Darts models\n",
    "model.fit(series=training_sets, val_series=validation_sets, num_loader_workers=AVAILABLE_CPUS//2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START = 3000\n",
    "for i, x in enumerate(sorted(glob.glob(\"../../../Data/london_clean/*.csv\"))[START:START+10]):\n",
    "\n",
    "    df = pd.read_csv(x)\n",
    "    df[\"DateTime\"] = pd.to_datetime(df['DateTime'])\n",
    "    series = TimeSeries.from_dataframe(df, value_cols=['KWHhh'], time_col=\"DateTime\", fill_missing_dates=True, freq=\"30min\").astype(np.float32)\n",
    "    series = series[-500:]\n",
    "\n",
    "\n",
    "    pred_series = model.historical_forecasts(\n",
    "        series,\n",
    "        forecast_horizon=1,\n",
    "        stride=1,\n",
    "        retrain=False,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    print(f\"rmse: {rmse(series, pred_series)}.\")\n",
    "    print(f\"R2 score: {r2_score(series, pred_series)}.\")\n",
    "\n",
    "    helper.display_forecast(pred_series, series, \"1 day\", save=True, fig_name=f\"{x.split('/')[-1]}-test\", fig_size=(20,10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START = 3000\n",
    "for i, x in enumerate(sorted(glob.glob(\"../../../Data/london_clean/*.csv\"))[START:START+10]):\n",
    "\n",
    "    df = pd.read_csv(x)\n",
    "    df[\"DateTime\"] = pd.to_datetime(df['DateTime'])\n",
    "    series = TimeSeries.from_dataframe(df, value_cols=['KWHhh'], time_col=\"DateTime\", fill_missing_dates=True, freq=\"30min\").astype(np.float32)\n",
    "    series = series[-500:]\n",
    "\n",
    "\n",
    "    pred_series = model.predict(\n",
    "        1,\n",
    "        series,\n",
    "    )\n",
    "\n",
    "    print(f\"rmse: {rmse(series, pred_series)}.\")\n",
    "    print(f\"R2 score: {r2_score(series, pred_series)}.\")\n",
    "\n",
    "    helper.display_forecast(pred_series, series, \"1 day\", save=True, fig_name=f\"{i}-test\", model_name=f\"{MODEL_NAME}\", fig_size=(20,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading checkpoints of the model\n",
    "\n",
    "loading the best checkpoint of the model. To compare the results of the model with the previous one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "model = TFTModel.load_from_checkpoint(work_dir=\"../../../Models/\", model_name=MODEL_NAME, best=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TFTModel.load_model(f\"../../../Models/{MODEL_NAME}/_model.pth.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
