{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"tft-nb-4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from multiprocessing.dummy import freeze_support\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = 'pytorch_stats_own_data.ipynb'\n",
    "os.environ['WANDB_API_KEY'] = os.getenv('WANDB_API_KEY')\n",
    "\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "from darts import TimeSeries\n",
    "from darts.models import TFTModel\n",
    "from darts.dataprocessing.transformers import Scaler, MissingValuesFiller\n",
    "from darts.utils.likelihood_models import QuantileRegression\n",
    "from darts.metrics import mape, r2_score, rmse, mse\n",
    "from darts import TimeSeries\n",
    "\n",
    "import helper\n",
    "import glob\n",
    "\n",
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.strategies import DDPStrategy, ddp2\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from tqdm.contrib.concurrent import process_map\n",
    "import tqdm\n",
    "\n",
    "\n",
    "AVAILABLE_GPUS = torch.cuda.device_count()\n",
    "AVAILABLE_CPUS = os.cpu_count()\n",
    "TRAINING_DATA_PATH = \"../../../Data/london_clean/*.csv\"\n",
    "\n",
    "print(f\"Available GPUs: {AVAILABLE_GPUS}\")\n",
    "print(f\"Available CPUs: {AVAILABLE_CPUS}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "Taking the first 300 households from the London Dataset and converting them to a Darts TimeSeries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_time_series_dataset = []\n",
    "# my_cov_series = []\n",
    "# for i in tqdm.tqdm(sorted(glob.glob(\"../../../Data/london_clean/*.csv\"))[:100]):\n",
    "#     df = pd.read_csv(\"../../../Data/London_weather_2011-2014.csv\")\n",
    "#     df[\"DateTime\"] = pd.to_datetime(df[\"DateTime\"])\n",
    "#     df2 = pd.read_csv(i)\n",
    "#     df2[\"DateTime\"] = pd.to_datetime(df2[\"DateTime\"])\n",
    "#     df = df.merge(df2, on=\"DateTime\", how=\"right\")\n",
    "#     df.fillna(method=\"ffill\" ,inplace=True)\n",
    "#     series = TimeSeries.from_dataframe(df, time_col=\"DateTime\", value_cols=[\"KWHhh\"], freq=\"30min\", fill_missing_dates=True, fillna_value=True).astype(np.float32)\n",
    "#     cov_series =  TimeSeries.from_dataframe(df, time_col=\"DateTime\", value_cols=[\"Temperature_C\", \"Humidity_%\", \"Dew_Point_C\"], freq=\"30min\", fill_missing_dates=True, fillna_value=True).astype(np.float32)\n",
    "#     my_time_series_dataset.append(series)\n",
    "#     my_cov_series.append(cov_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "weather = pd.read_csv(\"../../../Data/London_weather_2011-2014.csv\")\n",
    "weather[\"DateTime\"] = pd.to_datetime(weather[\"DateTime\"])\n",
    "\n",
    "def reader(x):\n",
    "    df2 = pd.read_csv(x)\n",
    "    df2[\"DateTime\"] = pd.to_datetime(df2[\"DateTime\"])\n",
    "    df = weather.merge(df2, on=\"DateTime\", how=\"right\")\n",
    "    df.fillna(method=\"ffill\" ,inplace=True)\n",
    "    series = TimeSeries.from_dataframe(df, time_col=\"DateTime\", value_cols=[\"KWHhh\"], freq=\"30min\", fill_missing_dates=True, fillna_value=True).astype(np.float32)\n",
    "    covarient = TimeSeries.from_dataframe(df, time_col=\"DateTime\", value_cols=[\"Temperature_C\", \"Humidity_%\", \"Dew_Point_C\"], freq=\"30min\", fill_missing_dates=True, fillna_value=True).astype(np.float32)\n",
    "    list = [series, covarient]\n",
    "    return list\n",
    "\n",
    "\n",
    "def splitter():\n",
    "    file_list = sorted(glob.glob(\"../../../Data/london_clean/*.csv\"))[:1000]\n",
    "    if file_list == []:\n",
    "        raise Exception(\"No files found\")\n",
    "    return process_map(reader, file_list, chunksize=20)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    freeze_support()\n",
    "    my_time_series_dataset = splitter()\n",
    "\n",
    "    values_series = [x[0] for x in my_time_series_dataset]\n",
    "    covariance_series = [x[1] for x in my_time_series_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_series[19].plot()\n",
    "covariance_series[19].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sets\n",
    "training_sets = []\n",
    "validation_sets = []\n",
    "for x in values_series:\n",
    "    train, val = x.split_after(0.85)\n",
    "    training_sets.append(train)\n",
    "    validation_sets.append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##normalize the data\n",
    "scaler = Scaler()\n",
    "\n",
    "train_transformed = []\n",
    "val_transformed = []\n",
    "for x in training_sets:\n",
    "    train_transformed.append(scaler.fit_transform(x))\n",
    "for x in validation_sets:\n",
    "    val_transformed.append(scaler.fit_transform(x))\n",
    "\n",
    "\n",
    "## covariate series\n",
    "cov_transformed = []\n",
    "for x in covariance_series:\n",
    "    cov_transformed.append(scaler.fit_transform(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transformed[19].plot()\n",
    "val_transformed[19].plot()\n",
    "cov_transformed[19].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "We create Temporal Fusion Transformer model that utilizes the GPU, Weights, Biases logger and early stopping callback.\n",
    "\n",
    "## Early stopping\n",
    "\n",
    "An early stopping callback is used to stop the training if the validation loss does not improve after a certain number of epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=0.1,\n",
    "    patience=10,\n",
    "    verbose=True,\n",
    "    mode=\"min\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowest_q, low_q, high_q, highest_q = 0.01, 0.1, 0.9, 0.99\n",
    "label_q_outer = f\"{int(lowest_q * 100)}-{int(highest_q * 100)}th percentiles\"\n",
    "label_q_inner = f\"{int(low_q * 100)}-{int(high_q * 100)}th percentiles\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoders = {\n",
    "    # \"datetime_attribute\": {\"future\": [\"DateTime\"], \"past\": [\"DateTime\"]},\n",
    "    \"position\": {\"past\": [\"absolute\"], \"future\": [\"relative\"]},\n",
    "    \"transformer\": Scaler(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_logger = WandbLogger(project=\"Digital-Energy\", log_model=True)\n",
    "\n",
    "\n",
    "# input chunk = The length of the input sequence fed to the model\n",
    "# output chunk = The length of the output sequence predicted by the model\n",
    "\n",
    "\n",
    "# default quantiles for QuantileRegression\n",
    "quantiles = [\n",
    "    0.01,\n",
    "    0.05,\n",
    "    0.1,\n",
    "    0.15,\n",
    "    0.2,\n",
    "    0.25,\n",
    "    0.3,\n",
    "    0.4,\n",
    "    0.5,\n",
    "    0.6,\n",
    "    0.7,\n",
    "    0.75,\n",
    "    0.8,\n",
    "    0.85,\n",
    "    0.9,\n",
    "    0.95,\n",
    "    0.99,\n",
    "]\n",
    "\n",
    "input_chunk_length = 96\n",
    "forecast_horizon = 96\n",
    "\n",
    "model = TFTModel(\n",
    "    input_chunk_length=input_chunk_length,\n",
    "    output_chunk_length=forecast_horizon,\n",
    "    hidden_size=64,\n",
    "    lstm_layers=2,\n",
    "    num_attention_heads=4,\n",
    "    dropout=0.04,\n",
    "    batch_size=512,\n",
    "    n_epochs=20,\n",
    "    add_relative_index=True,\n",
    "    add_encoders=encoders,\n",
    "    work_dir=\"../../../Models\",\n",
    "    save_checkpoints=False,\n",
    "    pl_trainer_kwargs={\n",
    "    \"enable_progress_bar\": True,\n",
    "    \"enable_model_summary\": True,\n",
    "    \"accelerator\": \"gpu\",\n",
    "    \"devices\": [1],\n",
    "    \"logger\": wandb_logger,\n",
    "    \"callbacks\": [early_stop_callback]\n",
    "    },\n",
    "    # likelihood=QuantileRegression(\n",
    "    #     quantiles=quantiles\n",
    "    # ),  # QuantileRegression is set per default\n",
    "    random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wandb_logger.watch(model_nbeats) # sadly this feature does not work for Darts models\n",
    "model.fit(series=train_transformed, val_series=val_transformed,  num_loader_workers=AVAILABLE_CPUS, future_covariates=cov_transformed, val_future_covariates=cov_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START = 3000\n",
    "weather = pd.read_csv(\"../../../Data/London_weather_2011-2014.csv\")\n",
    "weather[\"DateTime\"] = pd.to_datetime(weather[\"DateTime\"]) \n",
    "for i, x in enumerate(sorted(glob.glob(\"../../../Data/london_clean/*.csv\"))[START:START+1]):\n",
    "    df = pd.read_csv(x)\n",
    "    df[\"DateTime\"] = pd.to_datetime(df[\"DateTime\"])\n",
    "    df = weather.merge(df, on=\"DateTime\", how=\"right\")\n",
    "    df.fillna(method=\"ffill\" ,inplace=True)\n",
    "    series = TimeSeries.from_dataframe(df, time_col=\"DateTime\", value_cols=[\"KWHhh\"], freq=\"30min\", fill_missing_dates=True, fillna_value=True).astype(np.float32)\n",
    "    covarient = TimeSeries.from_dataframe(df, time_col=\"DateTime\", value_cols=[\"Temperature_C\", \"Humidity_%\", \"Dew_Point_C\"], freq=\"30min\", fill_missing_dates=True, fillna_value=True).astype(np.float32)\n",
    "    series = series[5000:5250]\n",
    "\n",
    "\n",
    "    pred_series = model.historical_forecasts(\n",
    "        series,\n",
    "        future_covariates=covarient,\n",
    "        forecast_horizon=1,\n",
    "        stride=1,\n",
    "        retrain=False,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    print(f\"rmse: {rmse(series, pred_series)}.\")\n",
    "    print(f\"R2 score: {r2_score(series, pred_series)}.\")\n",
    "\n",
    "    # helper.display_forecast(pred_series, series, \"1 day\", save=True, fig_name=f\"{x.split('/')[-1]}-test\", fig_size=(20,10))\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    pred_series.plot(low_quantile=lowest_q, high_quantile=highest_q, label=label_q_outer)\n",
    "    series.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(1, series, future_covariates=covarient, num_samples=50)\n",
    "series.plot()\n",
    "preds.plot()\n",
    "plt.legend()\n",
    "# save the plot\n",
    "plt.savefig(\"../../../Plots/TFT_weather5.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "series.plot()\n",
    "pred_series.plot(low_quantile=lowest_q, high_quantile=highest_q, label=label_q_outer)\n",
    "plt.legend()\n",
    "# save the plot\n",
    "plt.savefig(\"../../../Plots/TFT_weather3.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"rmse: {rmse(series, pred_series)}.\")\n",
    "print(f\"R2 score: {r2_score(series, pred_series)}.\")\n",
    "\n",
    "fig = helper.display_forecast(pred_series, series, \"1 day\", save=True, fig_name=f\"{i}\", model_name=\"test\", fig_size=(20,10))\n",
    "\n",
    "wandb.log({\n",
    "        \"mape\": mape(series, pred_series),\n",
    "        \"mse\": mse(series, pred_series),\n",
    "        \"rmse\": rmse(series, pred_series),\n",
    "        \"r2\": r2_score(series, pred_series),\n",
    "        \"result\": fig\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START = 3000\n",
    "SAMPLES = 5\n",
    "\n",
    "weather = pd.read_csv(\"../../../Data/London_weather_2011-2014.csv\")\n",
    "weather[\"DateTime\"] = pd.to_datetime(weather[\"DateTime\"])\n",
    "scaler = Scaler()\n",
    "\n",
    "lowest_q, low_q, high_q, highest_q = 0.01, 0.1, 0.9, 0.99\n",
    "label_q_outer = f\"{int(lowest_q * 100)}-{int(highest_q * 100)}th percentiles\"\n",
    "label_q_inner = f\"{int(low_q * 100)}-{int(high_q * 100)}th percentiles\"\n",
    "\n",
    "for i, x in enumerate(sorted(glob.glob(TRAINING_DATA_PATH))[START:START+SAMPLES]):\n",
    "\n",
    "    df = pd.read_csv(x)\n",
    "    df[\"DateTime\"] = pd.to_datetime(df[\"DateTime\"])\n",
    "    df = weather.merge(df, on=\"DateTime\", how=\"right\")\n",
    "    df.fillna(method=\"ffill\" ,inplace=True)\n",
    "    series = TimeSeries.from_dataframe(df, time_col=\"DateTime\", value_cols=[\"KWHhh\"], freq=\"30min\", fill_missing_dates=True, fillna_value=True).astype(np.float32)\n",
    "    covarient = TimeSeries.from_dataframe(df, time_col=\"DateTime\", value_cols=[\"Temperature_C\", \"Humidity_%\", \"Dew_Point_C\"], freq=\"30min\", fill_missing_dates=True, fillna_value=True).astype(np.float32)\n",
    "    MID = len(series)//2\n",
    "    series = series[MID:MID+600]\n",
    "    series = scaler.fit_transform(series)\n",
    "\n",
    "\n",
    "    pred_series = model.historical_forecasts(\n",
    "        series,\n",
    "        future_covariates=covarient,\n",
    "        forecast_horizon=96,\n",
    "        stride=1,\n",
    "        retrain=False,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    print(f\"rmse: {rmse(series, pred_series)}.\")\n",
    "    print(f\"R2 score: {r2_score(series, pred_series)}.\")\n",
    "\n",
    "    # fig = helper.display_forecast(pred_series, series, \"1 day\", save=False, fig_name=f\"{x.split('/')[-1]}-test\", fig_size=(20,10))\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.ylim(0, 3)\n",
    "    plt.gca().set_alpha(1)\n",
    "    series.plot()\n",
    "    plt.title(f\"{MODEL_NAME} - {x.split('/')[-1]} - MSE: {mse(series, pred_series)}\")\n",
    "    pred_series.plot(low_quantile=lowest_q, high_quantile=highest_q, label=label_q_outer)\n",
    "    fig = plt\n",
    "    # wandb.log({\n",
    "    #         \"mape\": mape(series, pred_series),\n",
    "    #         \"mse\": mse(series, pred_series),\n",
    "    #         \"rmse\": rmse(series, pred_series),\n",
    "    #         \"r2\": r2_score(series, pred_series),\n",
    "    #         \"result\": fig\n",
    "    # })\n",
    "    name = x.split('/')[-1].split('.')[0]\n",
    "    if not os.path.exists(f\"../../../Plots/{MODEL_NAME}/\"):\n",
    "        os.makedirs(f\"../../../Plots/{MODEL_NAME}/\")\n",
    "    plt.savefig(f\"../../../Plots/{MODEL_NAME}/{name}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading checkpoints of the model\n",
    "\n",
    "loading the best checkpoint of the model. To compare the results of the model with the previous one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"tft-2000-3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "model = TFTModel.load_from_checkpoint(work_dir=\"../../../Models/\", model_name=MODEL_NAME, best=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.trainer_params.update({\"accelerator\": \"cpu\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.eval(model, future_covariates=True, data_normalized=True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
